# Training Configuration
model_type: transformer  # lstm or transformer

data:
  sequence_length: 5
  test_size: 0.2
  val_size: 0.1

training:
  batch_size: 64
  learning_rate: 0.001
  weight_decay: 0.00001
  epochs: 100
  optimizer: adamw  # adam or adamw
  scheduler: cosine  # cosine or reduce_on_plateau
  clip_grad: 1.0
  early_stopping: true
  early_stopping_patience: 10

mlflow:
  experiment_name: f1_race_prediction
  tracking_uri: ./mlruns
